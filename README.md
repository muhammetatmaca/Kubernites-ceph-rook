# Kubernetes & Ceph Entegrasyon Projesi

### ğŸš€ Proje HakkÄ±nda
Bu proje, yÃ¼ksek eriÅŸilebilirlik ve Ã¶lÃ§eklenebilirlik saÄŸlamak amacÄ±yla bir **Ceph daÄŸÄ±tÄ±k depolama kÃ¼mesinin**, **Kubernetes** ortamÄ± Ã¼zerine kurulmasÄ±nÄ± ve entegrasyonunu ele alÄ±r. GeliÅŸtirilen basit bir API uygulamasÄ±, Ceph'in saÄŸladÄ±ÄŸÄ± kalÄ±cÄ± depolama alanÄ±nÄ± (Persistent Volume Claim - PVC) kullanarak verilerini kalÄ±cÄ± hale getirir.

Proje, daÄŸÄ±tÄ±k sistemler ve bulut teknolojileri alanÄ±ndaki teorik bilgileri pratik bir senaryo ile pekiÅŸtirmeyi hedefler.

---

### ğŸŒŸ Proje Topolojisi
Proje, bir Master ve iki Worker dÃ¼ÄŸÃ¼mÃ¼nden oluÅŸan basit bir kÃ¼me topolojisine sahiptir.

| Rol | Makine AdÄ± | IP Adresi | RAM | Diskler |
| :--- | :--- | :--- | :--- | :--- |
| **Master** | k8s-master | 192.168.175.129 | 2 GB | 20 GB |
| **Node 1** | k8s-node-1 | 192.168.175.131 | 4 GB | 40 GB + **100 GB** |
| **Node 2** | k8s-node-2 | 192.168.175.130 | 4 GB | 40 GB + **100 GB** |

*Her bir Worker dÃ¼ÄŸÃ¼mÃ¼ndeki **100 GB'lÄ±k ikinci diskler**, Ceph depolama havuzu iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.*

---

### ğŸ“¦ BÃ¶lÃ¼m 1: AltyapÄ± ve Kubernetes Kurulumu
Bu bÃ¶lÃ¼mde, proje iÃ§in gerekli sanal makineler kurulmuÅŸ ve Kubernetes kÃ¼mesi yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r.

1.  **Sanal Makine Kurulumu:** VirtualBox Ã¼zerinde master ve iki worker makinesi oluÅŸturuldu. Worker makinelerine Ceph iÃ§in ek diskler eklendi.
2.  **Statik IP AtamasÄ±:** TÃ¼m makinelere sabit IP adresleri atanarak aÄŸ iletiÅŸimi saÄŸlandÄ±.
3.  **Ortak Paket KurulumlarÄ±:** TÃ¼m makinelere `containerd`, `kubeadm`, `kubelet` ve `kubectl` gibi temel paketler kuruldu.
4.  **Kubernetes KÃ¼me BaÅŸlatma:** `kubeadm init` komutu ile master dÃ¼ÄŸÃ¼mde kÃ¼me baÅŸlatÄ±ldÄ± ve `kubeadm join` ile worker dÃ¼ÄŸÃ¼mleri kÃ¼meye dahil edildi. `Flannel` CNI (Container Network Interface) eklentisi kuruldu.

---

### ğŸ“ BÃ¶lÃ¼m 2: API UygulamasÄ± ve DaÄŸÄ±tÄ±m
Bu aÅŸamada, dosya yÃ¼kleme ve indirme iÅŸlemleri iÃ§in basit bir Python API geliÅŸtirildi ve Kubernetes'te Ã§alÄ±ÅŸtÄ±rÄ±lmaya hazÄ±r hale getirildi.

1.  **API GeliÅŸtirme:** Base64 formatÄ±nda veri alÄ±p kaydeden bir **Flask** API (`app.py`) oluÅŸturuldu.
2.  **Dockerizasyon:** Uygulama, `Dockerfile` ile bir Docker imajÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ (`my-simple-api:v1`).
3.  **Kubernetes DaÄŸÄ±tÄ±mÄ±:** Uygulama, bir **Deployment** ve **Service** objeleri kullanÄ±larak Kubernetes Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±.

---

### ğŸ›¡ï¸ BÃ¶lÃ¼m 3: Ceph ve KalÄ±cÄ± Depolama
Bu kÄ±sÄ±m, projenin kalbi olan Ceph entegrasyonunu ve Minio kurulumunu iÃ§erir.

1.  **Rook OperatÃ¶rÃ¼ ile Ceph Kurulumu:** **Rook** operatÃ¶rÃ¼, Ceph'in Kubernetes'e kolayca daÄŸÄ±tÄ±lmasÄ± ve yÃ¶netilmesi iÃ§in kullanÄ±ldÄ±. `cluster.yaml` dosyasÄ± ile nodelardaki diskler otomatik olarak Ceph depolamasÄ±na dahil edildi.
2.  **KalÄ±cÄ± Depolama (PVC):** API uygulamasÄ±, Ceph'in saÄŸladÄ±ÄŸÄ± depolama alanÄ±nÄ± bir `PersistentVolumeClaim` (PVC) ile talep etti. Bu sayede, pod'lar yeniden baÅŸlatÄ±lsa bile veriler kalÄ±cÄ± oldu.
3.  **Minio Kurulumu:** Ceph'in Ã¼zerine, S3 uyumlu bir obje depolama Ã§Ã¶zÃ¼mÃ¼ olan **Minio** kuruldu. Minio da kendi verileri iÃ§in Ceph'in kalÄ±cÄ± depolama alanÄ±nÄ± kullandÄ±.

---

### âœ… Testler ve SonuÃ§lar
* API'ye `cURL` komutlarÄ± ile dosya yÃ¼kleme ve indirme iÅŸlemleri yapÄ±ldÄ±.
* API'nin pod'u silinip yeniden oluÅŸturularak kalÄ±cÄ± depolama testi baÅŸarÄ±yla gerÃ§ekleÅŸtirildi. DosyalarÄ±n kaybolmadÄ±ÄŸÄ± teyit edildi.
* Minio web arayÃ¼zÃ¼ne eriÅŸilerek obje depolama iÅŸlevselliÄŸi test edildi.
